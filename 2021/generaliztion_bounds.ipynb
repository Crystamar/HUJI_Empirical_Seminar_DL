{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-05-29T08:26:17.722318Z","iopub.execute_input":"2021-05-29T08:26:17.722888Z","iopub.status.idle":"2021-05-29T08:26:17.739711Z","shell.execute_reply.started":"2021-05-29T08:26:17.722804Z","shell.execute_reply":"2021-05-29T08:26:17.738456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# save the final model to file\nimport keras\nimport numpy as np\nfrom keras.datasets import fashion_mnist\nfrom keras.utils.np_utils import to_categorical\n\nfrom keras.models import Sequential\nfrom keras.layers import Conv2D\nfrom keras.layers import MaxPooling2D\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.optimizers import SGD\n\n# load train and test dataset\ndef load_dataset():\n\t# load dataset\n\t(trainX, trainY), (testX, testY) = fashion_mnist.load_data()\n\t# one hot encode target values\n\ttrainY = keras.utils.np_utils.to_categorical(trainY)\n\ttestY = keras.utils.np_utils.to_categorical(testY)\n\treturn trainX, trainY, testX, testY\n\n# scale pixels\ndef prep_pixels(train, test):\n\t# convert from integers to floats\n\ttrain_norm = train.astype('float32')\n\ttest_norm = test.astype('float32')\n\t# normalize to range 0-1\n\ttrain_norm = train_norm / 255.0\n\ttest_norm = test_norm / 255.0\n\t# return normalized images\n\treturn train_norm, test_norm\n\n# define cnn model\ndef define_model(learn=0.001,moment=0.9):\n\tmodel = Sequential()\n\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n\tmodel.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n\tmodel.add(MaxPooling2D((2, 2)))\n\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n\tmodel.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n\tmodel.add(MaxPooling2D((2, 2)))\n\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n\tmodel.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n\tmodel.add(MaxPooling2D((2, 2)))\n\tmodel.add(Flatten())\n\tmodel.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n\tmodel.add(Dense(10, activation='softmax'))\n\t# compile model\n\topt = SGD(lr=learn, momentum=moment)\n\tmodel.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n\treturn model\n\n# run the test harness for evaluating a model\ndef run_test_harness():\n\t# load dataset\n\ttrainX, trainY, testX, testY = load_dataset()\n\t# prepare pixel data\n\ttrainX, testX = prep_pixels(trainX, testX)\n\t# define model\n\tmodel = define_model()\n\t# fit model\n\tmodel.fit(trainX, trainY, epochs=100, batch_size=64, verbose=1)\n\t# save model\n\tmodel.save('final_model.h5')\n\n# entry point, run the test harness\n# run_test_harness()","metadata":{"execution":{"iopub.status.busy":"2021-05-29T08:28:41.663082Z","iopub.execute_input":"2021-05-29T08:28:41.66348Z","iopub.status.idle":"2021-05-29T08:28:47.467987Z","shell.execute_reply.started":"2021-05-29T08:28:41.663398Z","shell.execute_reply":"2021-05-29T08:28:47.466873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef train_one_model(learn=0.001,moment=0.9,bs=64,i=0):\n  # load dataset\n  trainX, trainY, testX, testY = load_dataset()\n  # prepare pixel data\n  trainX, testX = prep_pixels(trainX, testX)\n  # define model\n  model = define_model(learn,moment)\n  # fit model\n  model.fit(trainX, trainY, epochs=1, batch_size=bs, verbose=1)\n  while model.evaluate( trainX, trainY,batch_size=bs)[0]>0.1:\n    model.fit(trainX, trainY, epochs=1, batch_size=bs, verbose=1)\n\n  # save model\n  model.save('fm_model_'+str(i)+'_learn'+str(learn)+'_batch'+str(bs)+'.h5')\n  del model\n# train_one_model()","metadata":{"execution":{"iopub.status.busy":"2021-05-29T08:28:47.469893Z","iopub.execute_input":"2021-05-29T08:28:47.470319Z","iopub.status.idle":"2021-05-29T08:28:47.478262Z","shell.execute_reply.started":"2021-05-29T08:28:47.470264Z","shell.execute_reply":"2021-05-29T08:28:47.476902Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bss = [64,128]\nlrs = [0.003,0.001]","metadata":{"execution":{"iopub.status.busy":"2021-05-29T08:28:47.480759Z","iopub.execute_input":"2021-05-29T08:28:47.481691Z","iopub.status.idle":"2021-05-29T08:28:47.495145Z","shell.execute_reply.started":"2021-05-29T08:28:47.481479Z","shell.execute_reply":"2021-05-29T08:28:47.49392Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for bsk in bss:\n    for lr in lrs:\n        for j in range(5):\n            train_one_model(learn=lr,moment=0.9,bs=bsk,i=j)","metadata":{"execution":{"iopub.status.busy":"2021-05-29T08:28:48.783518Z","iopub.execute_input":"2021-05-29T08:28:48.783895Z","iopub.status.idle":"2021-05-29T08:28:53.687838Z","shell.execute_reply.started":"2021-05-29T08:28:48.783864Z","shell.execute_reply":"2021-05-29T08:28:53.683948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras import backend as k\ntf.compat.v1.disable_eager_execution()\n\ndef var_grad(mod,X,y):\n    outputTensor = model.output #Or model.layers[index].output\n    listOfVariableTensors = model.trainable_weights\n    gradients = k.gradients(outputTensor, listOfVariableTensors)\n    sess = tf.compat.v1.InteractiveSession()\n    sess.run(tf.compat.v1.initialize_all_variables())\n    evaluated_gradients = sess.run(gradients,feed_dict={model.input:X})\n\n    print(evaluated_gradients)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\n# load dataset\ntrainX, trainY, testX, testY = load_dataset()\n# prepare pixel data\ntrainX, testX = prep_pixels(trainX, testX)\n\nmodel = load_model('../input/models/model_0_learn0.001_batch128.h5')\nvar_grad(model,trainX[:5000],trainY[:5000])\n# pacbayes_sigma(model_fn=model,image_tensor=trainX[:5000], label_tensor=trainY[:5000], training_accuracy=model.evaluate(trainX, trainY)[1], target_deviate=0.1,\n#     checkpoint_directory='../input/models')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras import backend as k\n\nfrom keras.models import load_model\n# load dataset\ntrainX, trainY, testX, testY = load_dataset()\n# prepare pixel data\ntrainX, testX = prep_pixels(trainX, testX)\ndef unpack(a):\n    b = []\n    for i in range(len(a)):\n        if type(a[i]) == type(a[0]):\n            for j in a[i]:\n                b.append(j)\n            continue\n        if type(a[i]) == type(a[30]):\n            b.append(a[i])\n            continue\n    return b\n\nvariances = np.array((2,2,5))\ngaps = np.array((2,2,5))\nj=0\nt=0\n\nfor bsk in bss:\n    for lr in lrs:\n        for i in range(5):\n            print('model_'+str(i)+'_learn'+str(lr)+'_batch'+str(bsk)+'.h5')\n            model = load_model('../input/models/model_'+str(i)+'_learn'+str(lr)+'_batch'+str(bsk)+'.h5')\n            trainX = trainX[:5000]\n            outputTensor = model.output #Or model.layers[index].output\n\n            listOfVariableTensors = model.trainable_weights\n            gradients = k.gradients(outputTensor, listOfVariableTensors)\n            sess = tf.compat.v1.InteractiveSession()\n            sess.run(tf.compat.v1.initialize_all_variables())\n            evaluated_gradients = sess.run(gradients,feed_dict={model.input:trainX})\n            \n            a=[item for sublist in evaluated_gradients for item in sublist]\n            print(np.var(unpack(unpack(unpack(a)))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"type(a[0])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def unpack(a):\n    b = []\n    for i in range(len(a)):\n        if type(a[i]) == type(a[0]):\n            for j in a[i]:\n                b.append(j)\n            continue\n        if type(a[i]) == type(a[30]):\n            b.append(a[i])\n            continue\n    return b\nfor bsk in bss:\n    for lr in lrs:\n        for j in range(5):\n            train_one_model(learn=lr,moment=0.9,bs=bsk,i=j)\nprint(np.var(unpack(unpack(unpack(a)))))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"variances = np.zeros(shape=(2,2,5))\ngaps = np.zeros(shape=(2,2,5))\nt=0\nfor bsk in bss:\n    j=0\n    for lr in lrs:\n        for i in range(5):\n            print(i)\n            print(j)\n            print(t)\n            variances[t][j][i] = 0\n            gaps[t][j][i] = 0\n        j+=1\n    t+=1\n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation\nfrom keras import backend as k\nvariances = np.zeros(shape=(2,2,5))\ngaps = np.zeros(shape=(2,2,5))\nj=0\nt=0\nfor bsk in bss:\n    j=0\n    for lr in lrs:\n        for i in range(5):\n            # load dataset\n            trainX, trainY, testX, testY = load_dataset()\n            # prepare pixel data\n            trainX, testX = prep_pixels(trainX, testX)\n            print('model_'+str(i)+'_learn'+str(lr)+'_batch'+str(bsk)+'.h5')\n            model = load_model('../input/models/model_'+str(i)+'_learn'+str(lr)+'_batch'+str(bsk)+'.h5')\n            print(model.evaluate( trainX, trainY,batch_size=128)[1])\n            print(model.evaluate( testX, testY,batch_size=128)[1])\n\n            gap = model.evaluate( trainX, trainY,batch_size=128)[1]-model.evaluate( testX, testY,batch_size=128)[1]\n            trainX = trainX[:5000]\n            outputTensor = model.output #Or model.layers[index].output\n\n            listOfVariableTensors = model.trainable_weights\n# listOfVariableTensors = model.trainable_weights[-1]\n#or variableTensors = model.trainable_weights[0]\n            gradients = k.gradients(outputTensor, listOfVariableTensors)\n            sess = tf.compat.v1.InteractiveSession()\n            sess.run(tf.compat.v1.initialize_all_variables())\n            evaluated_gradients = sess.run(gradients,feed_dict={model.input:trainX})\n            \n            a=[item for sublist in evaluated_gradients for item in sublist]\n            var = np.var(unpack(unpack(unpack(a))))\n            variances[t][j][i] = var\n            gaps[t][j][i] = gap\n            print('var is :' + str(var))\n            print('gap is :' + str(gap))\n        j+=1\n    t+=1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gaps","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tao = np.zeros(shape=(5))\nfor i in range(5):\n    sumi = 0\n    for t in range(2):\n        for j in range(2):\n            u1 = variances[t][j][i]\n            g1 = gaps[t][j][i]\n\n            for p in range(2):\n                for d in range(2):\n                    if(p==t and d==j):\n                        continue\n                    u2 = variances[p][d][i]\n                    g2 = gaps[p][d][i]\n                    sumi+=np.sign(u1-u2)*np.sign(g1-g2)\n    tao[i] = (1/(4*3))*sumi","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tao","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.mean(tao)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_theme(style=\"whitegrid\")\nsns.boxplot(data=tao,orient='v',showmeans=True,meanline=True,bootstrap=10000,notch=True)\nplt.title('Weight gradients Complexity Measure Tao rank')\nplt.xlabel('Tao rank')\nplt.savefig('kendal')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mi=2.0\nn=2.0\npsbatch = np.zeros(shape=(5))\npslearn = np.zeros(shape=(5))\ngranu_kendal = np.zeros(shape=(5))\nfor i in range(5):\n    sumi=0\n    for j in range(2):\n        u1 = variances[0][j][i]\n        g1 = gaps[0][j][i]\n        u2 = variances[1][j][i]\n        g2 = gaps[1][j][i]\n        sumi+=np.sign(u1-u2)*np.sign(g1-g2)\n    psbatch[i] = (1/mi)*sumi\n\nfor i in range(5):\n    sumi=0\n    for j in range(2):\n        u1 = variances[j][0][i]\n        g1 = gaps[j][0][i]\n        u2 = variances[j][1][i]\n        g2 = gaps[j][1][i]\n        sumi+=np.sign(u1-u2)*np.sign(g1-g2)\n    pslearn[i] = (1/mi)*sumi\ngranu_kendal = (1/n)*(psbatch+pslearn)\nsns.set_theme(style=\"whitegrid\")\nsns.boxplot(data=granu_kendal,orient='v',showmeans=True,meanline=True,notch=True)\nplt.title('Weight gradients Complexity Measure Granulated Tao rank')\nplt.xlabel('Granulated Tao rank')\nplt.savefig('Granulatedkendal')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"granu_kendal","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}